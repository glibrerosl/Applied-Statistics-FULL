---
title: "**Admisión en la Universidad**"
subtitle: "Datos para ser admitido en estudios superiores en EE.UU."
author: "Por: Giancarlo Libreros Londoño::glibrerosl@libertadores.edu.co"
date: "Estudio Elaborado entre noviembre y diciembre de 2022 como actividad formativa y evaluativa del curso Análisis de Regresión de la especialización en Estadística Aplicada (modalidad virtual)."
output:
  html_document:
    toc: TRUE
    toc_float: TRUE
    code_download: TRUE
    theme: united
bibliography: bibliografia_ME.bib
csl: apa.csl
link-citations: yes
---
<!-- Configuración Global de R -->
```{r setup, include=FALSE}
library(readr)
library(readxl)
library(stargazer)

knitr::opts_chunk$set(echo=TRUE)

Admission_Dataset <- data.frame(read_excel("d:/ZB/[2] Academicas/[10.1] Especialización/[0] Cursos/[2] Análisis de Regresión/Tareas/Tarea_1/Admission_Dataset.xlsx"))
```

## **1. Objetivo del Estudio**
En términos generales, este estudio establecerá la relación entre dos o más variables a través de la obtención de información sobre una de ellas con base en el conocimiento de los valores de las demás. La relación que se establecerá entre ellas es de naturaleza no-determinística; es decir, se formularán relaciones probabilísticas y procedimientos para hacer inferencias sobre los modelos usados en este estudio, a la vez que se obtienen medidas cuantitativas del grado en el que las variables están relacionadas. Los modelos estudiados pueden verse como casos especialies del modelo lineal generalizado: Regresión Lineal Simple, Regresión Lineal Múltiple y Regresión Logística. En cada apartado se describirá teóricamente a cada uno y se usará como objeto de estudio un conjunto de datos en particular que es descrito en la sección **2**. Cabe anotar que los fundamentos teóricos expuestos provienen de notas de clase del curso Métodos Estadísticos dictado por el profesor Dagoberto Bermúdez para la Especialización en Estadística Aplicada, modalidad virtual (2022-4), de la Fundación Universitaria Los Libertadores; y de los libros probabilidad y estadística de Jay L. Devore [@PEDevore7ed], Bioestadística de Wayne W. Daniel [@BEDaniel4ed] y Métodos Matemáticos de Estadística de Harald Cramer [@MMECramer1ed].

## **2. Descripción de los Datos**

### 2.1. Fuente del Conjunto de Datos
El conjunto de datos de trabajo se obtuvo casi totalmente de **Kaggle**: https://www.kaggle.com/akshaydattatraykhare. Es conveniente anotar que **Kaggle** es una compañía subsidiaria de Google LLC que mantiene una comunidad online de científicos de datos y profesionales del aprendizaje automático. Esta empresa permite a sus usuarios encontrar y publicar conjuntos de datos, explorar y crear modelos en un entorno de ciencia de datos basado en la web, trabajar con otros científicos de datos e ingenieros de aprendizaje automático y participar en concursos para resolver desafíos de ciencia de datos.

### 2.2. Contexto del Conjunto de Datos
El conjunto de datos incluye métricas académicas obtenidas por estudiantes extranjeros para aspirar a acceder a universidades de EE.UU. Este conjunto de datos se actualizó por última vez en julio de 2022.

### 2.3. Descripción del Conjunto de Datos
El conjunto de datos contiene 10 campos y 400 registros. Uno de los campos es simplemente un identificador numérico secuencial de los registros; otros tres son de naturaleza politómica; y el resto son numéricos estrictamente positivos. La lista siguiente los describe en el mismo orden, de izquierdda a derecha, como aparecen en el rango de datos que los contiene y se establece para cada campo, excepto el campo **Serial**, el tipo de variable y su escala de medición con base en la nomenclatura (tipo_de_variable::escala_de_medición):

- **Serial** (identificador): registra un número secuenciado a partir de 1 para identificar de forma única cada registro consignado en el conjunto de datos.

- **Gender** (cualitativa::nominal): registra el sexo del estudiante del cual se registraron los datos: 1 corresponde con un estudiante de sexo masculino, 0 con un estudiante de sexo femenino.

- **GRE Score** (cuantitativa::razón): registra el puntaje total GRE (examen de acceso a la universidad) obtenido por el estudiante. GRE es un componente común del proceso de admisión a colegios o universidades en EE.UU. que mide el razonamiento verbal, cuantitativo, la escritura analítica y las habilidades de pensamiento crítico que se han adquirido a lo largo de un extenso período de tiempo y que no están relacionados con campo específicos de estudio. El campo solo registra dos de los tres componentes de la evaluación: razonamiento verbal y cuantitativo, en una escala desde 260 hasta 340 puntos. El resultado ausente del puntaje corresponde con el componente de escritura analítica: calificado entre 0 y 6 puntos.

- **TOEFL Score** (cuantitativa::razón): registra el puntaje total TOEFL (prueba de inglés como idioma extranjero) obtenido por el estudiante. TOEFL es un componente común del proceso de admisión a colegios o universidades en EE.UU. por parte de estudiantes extranjeros que mide las competencias en comprensión escrita, comprensión oral, expresión oral y expresión escrita, en una escala desde 0 hasta 120 puntos.

- **SOP** (cuantitativa::razón): registra el puntaje total SOP (ensayo de declaración de propósitos o de admisión) obtenido por el estudiante. SOP es un componente común del proceso de admisión a colegios o universidades en EE.UU. que consiste en un ensayo de solicitud de ingreso escrito por el estudiante en el cual debe hacer una descripción general de quién es, en quién quiere convertirse y hasta qué punto está preparado para seguir un determinado curso en la institución educativa a la cual aspira ingresar. Este ensayo se califica con un puntaje entre 0 y 5.

- **LOR** (cuantitativa::razón): registra el puntaje total LOR (carta de recomendación) obtenido por el estudiante. LOR es un componente común del proceso de admisión a colegios o universidades en EE.UU. que consiste en una recomendación escrita, generalmente por un profesor, en la cual el redactor evalúa las cualidades, características y capacidades del estudiante recomendado en relación con su aptitud para seguir un curso en la institución educativa a la cual el estudiante aspira a ingresar. Esta carta se califica con un puntaje entre 0 y 5.

- **CGPA** (cuantitativa::razón): registra el puntaje total CGPA (promedio de calificaciones acumulativo) obtenido por el estudiante. CGPA es un componente común del proceso de admisión a colegios o universidades en EE.UU. que mide el desempeño promedio del estudiante en su escolaridad previa a la solicitud de ingreso a la institución educativa siguiente de su preferencia. Este puntaje se mide entre 0 y 4; sin embargo, en el conjunto de datos fue convertido en una escala entre 0 y 10.

- **Research** (cualitativa::nominal): registra la experiencia en investigación que posee el estudiante: 1 corresponde con que el estudiante argumenta experiencia investigativa, 0 corresponde con que no-argumenta experiencia investigativa.

- **University Rating** (cualitativa::nominal(ordenada)): registra valoración de la universidad a la cual aspira a ingresar el estudiante. Esta valoración se hace en una escala entre 0 y 5 estrellas, cinco estrellas indica la mejor valoración.

- **Chance of Admit** (cuantitativa::razón): registra la probabilidad de que el estudiante sea admitido en la universidad de su preferencia con base en los datos registrados a su nombre, salvo su sexo. Esta probrabilidad se mide entre 0 y 1.

## **3. Análisis de Regresiones**
Se sabe que el análisis de regresión es un proceso de naturaleza estadística usado para estimar relaciones entre variables (una dependiente o de respuesta y otras independientes o predictoras) a través de técnicas de modelado y análisis que permiten entender cómo el valor de la variable dependiente varía al cambiar el valor de una o más variables independientes. Los modelos de análisis de regresión estudiados a través de este documento serán: lineal (simple y múltiple) y logístico, ellos entendidos como casos del modelo de regresión lineal generalizado.

### 3.1. Regresión Lineal Simple
Este modelo, que eventualmente será llamado en este estudio como **RLS**, está conformado por dos variables estadísticas $x$ y $Y$, donde $Y$ se asume que está influida por $x$. La relación está dada matemáticamente por: $$Y = \beta_0 + \beta_1 x + \varepsilon \hspace{10mm} \hspace{10mm}(1)$$ donde:

+ $Y$: es una variable de respuesta de naturaleza aleatoria.
+ $x$: es una variable predictora de naturaleza no aleatoria.
+ $\varepsilon$: es una variable aleatoria no observable.
+ $\beta_0$ y $\beta_1$: son parámetros reales desconocidos del modelo.

En comparación con el modelo lineal simple determinístico $y = \beta_0 + \beta_1 x$, el probablístico supone que el valor esperado de $Y$ es una función lineal de $x$, pero que con $x$ fija, la variable $Y$ difiere de su valor esperado en una cantidad aleatoria $\varepsilon$. Además, la cantidad $\varepsilon$ en la ecuación de modelo $(1)$ se supone normalmente distribuida con $E(\varepsilon)=0$ y $V(\varepsilon)=\sigma^2$. La variable aleatoria $\varepsilon$ también se conoce como término de error aleatorio o desviación aleatoria en el modelo.

Complementariamente, casi nunca serán conocidos los valores $\beta_0$, $\beta_1$ y $\sigma^2$, a cambio estará disponible una muestra de datos compuesta de pares ordenados $(x_1,y_1)... (x_n,y_n)$ con la que los parámetros del modelo y la línea de regresión verdadera pueden ser estimados, bajo el supuesto de independencia de las observaciones. Así, $y_i$ es el valor observado de una variable aleatoria $Y_i$, donde $Y_i=\beta_0+\beta_1x_i+\varepsilon_i$ y las $n$ desviaciones $\varepsilon_1$, $\varepsilon_2$, $...$, $\varepsilon_n$ son variables independientes.

De acuerdo con el modelo, los puntos observados estarán distribuidos aleatoriamente alrededor de la línea de regresión verdadera. En este sentido, la estimación de $y=\beta_0+\beta_1x$ deberá ser una línea que se ajuste lo mejor posible a los puntos muestra. Tal línea deberá poseer la característica de que las distancias verticales (desviaciones) de los puntos observados a la línea misma son *_pequeñas_*. La medida de la bondad de ajuste será la suma de los cuadrados de estas desviaciones. En consecuencia, la línea que mejor se ajusta será la que tenga la suma más pequeña posible de desviaciones al cuadrado. El resultado que implica las ideas expuestas se conoce como: principio de los mínimos cuadrados y se remonta a los matemáticos Carl Friedrich Gauss y Adrien-Marie Legendre, entre el último lustro del siglo XVIII y el primero del siglo XIX.

El principio de los mínimos cuadrados establece que la desviación vertical del punto $(x_i,y_i)$ con respecto a la línea $y=b_0+b_1x$ es $y_i-(b_0+b_1x)$ y la suma de las desviaciones verticales al cuadrado de los puntos $(x_i,y_i)$ a la línea es $f(b_0,b_1)=\sum_{i=1}^n (y_i-(b_0+b_1x_i))^2$. Así, las estimaciones puntuales de $\beta_0$ y $\beta_1$, representadas como $\hat{\beta}_0$ y $\hat{\beta}_1$ y llamadas estimaciones de mínimos cuadrados, son los valores que minimizan a $f(b_0,b_1)$; es decir, $f(\hat{\beta}_0,\hat{\beta}_1)\leq f(b_0,b_1)$ para cualesquiera $\beta_0$ y $\beta_1$. Por lo tanto, la línea de regresión estimada o línea de mínimos cuadrados es $y=\hat{\beta}_0+\hat{\beta}_1x$.

Luego de calcular y resolver las ecuaciones en derivadas parciales de $f(b_0,b_1)$ respecto a $b_0$ y $b_1$ igualadas a cero, se obtiene un sistemas de ecuaciones llamadas normales que son lineales en $b_0$ y $b_1$ y para las cuales, siempre que por lo menos dos de las $x_i$ sean diferentes, las estimaciones de los mínimos cuadrados son la única solución del sistema. En consecuencia, la estimación de los mínimos cuadrados de $\beta_1$ de la línea de regresión verdadera es: $$\hat{\beta}_1=\dfrac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=\dfrac{S_{xy}}{S_{xx}}\hspace{10mm}(2)$$ y la estimación de los mínimos cuadrados de $\beta_0$ de la línea de regresión verdadera es: $$\hat{\beta}_0=\dfrac{\sum_{i=1}^ny_i-\hat{\beta}_1\sum_{i=1}^nx_i}{n}=\bar{y}-\hat{\beta}_1\bar{x}\hspace{10mm}(3)$$ Para hacer los cálculos que las ecuaciones anteriores demandan es necesario reducir al mínimo los efectos de redondeo. También, antes de calcular $\hat{\beta}_1$ y $\hat{\beta}_0$ se debe examir gráficamente el conjunto de datos por usar para percibir la factibilidad de uso de un modelo probabilístico lineal, es decir, si gráficamente los puntos están lejos de tender a aglomerarse en torno a una línea recta con aproximadamente el mismo grado de dispersión de todas las $x_i$, entonces deben ser indagados otros modelos.

Es indispensable mencionar que la línea de mínimos cuadrados debe usarse restringidamente para predecir valores de $x$ **_lejanos_** del rango de los datos, porque la relación ajustada puede carecer de validez para ellos.

Ahora, el parámetro $\sigma^2$ que determina la cantidad de variabilidad es inherente en el modelo de regresión descrito: su valor conducirá a establecer que los valores observados estarán dispersos en mayor o menor medida en torno a la línea de regresión verdadera. Así, los residuos $y_i - \hat{y_i}$ son las desviaciones verticales con respecto a la línea estimada. Si todos los residuos son pequeños comparados con cero, entonces la variabilidad de los valores $y$ observados se debería en una elevada medida a la relación lineal entre $x$ y $y$, mientras que si los residuos son grandes comparados con cero, entonces queda sugerida una variabilidad inherente en $y$ con respecto a la cantidad debida a la relación lineal. Así, la estimación de $\sigma^2$ en un análisis de regresión está basada en el cálculo de la suma de cuadrados residuales (o suma de cuadrados del error SCE) que se reduce a: $$SCE=\sum_{i=1}^ny_i^2-\hat{\beta}_0\sum_{i=1}^ny_i-\hat{\beta}_1\sum_{i=1}^nx_iy_i\hspace{10mm}(4)$$ $$\hat\sigma^2=s^2=\dfrac{SCE}{n-2}\hspace{10mm}(5)$$ Si se ha entendido que la cantidad SCE establece una medida de cuánta variación de $y$ es inexplicada por el modelo; es decir, sin atribución a la relación lineal, se entenderá también que existe otra cantidad llamada la suma total de los cuadrados STC, que permite obtener una medida de la cantidad de variación total en los valores $y$ observados: $$STC=\sum_{i=1}^ny_i^2-\frac{(\sum_{i=1}^ny_i)^2}{n}\hspace{10mm}(6)$$ Si se formula la razón $SCE/STC$ se calcula la proporción de variación total inexplicada por el modelo de regresión lineal simple; por lo tanto, se llega a la definición del coeficiente de determinación $r^2$: $$r^2=1-\frac{SCE}{STC}\hspace{10mm}(7)$$ que se interpreta como la proporción de variación $y$ observada que puede ser explicada por el modelo de regresión lineal simple; es decir, aquella atribuida a una relación lineal aproximada entre $x$ y $y$: mientras más cercano a 1 sea $r^2$, más exitoso es el modelo de regresión lineal simple al explicar la variación de $y$. Una forma alternativa de calcular el coeficiente de determinación se basa en la suma de cuadrados debidad a la regresión SCR (o al modelo de regresión SCM), que es la cantidad de variación total que es explicada por el modelo. Con base en ella el coeficiente de determinación se expresa como: $$r^2=1-\frac{SCE}{STC}=\frac{STC-SCE}{STC}=\frac{SCR}{STC}\hspace{10mm}(8)$$Como se sabe, cualquier cantidad calculada a partir de datos muestrales varía de una cantidad a otra, en este sentido, los procedimientos inferenciales estandarizan un estimador restando su valor medio y luego dividiéndolo entre su desviación estándar estimada. En particular, para un modelo supuesto de regresión lineal simple se implica que las variables estándares: $t_{(n-2)}=\dfrac{\hat{\beta}_0-\beta_0}{\hat{\sigma} \sqrt{1/n+\bar{x}^2/S_{xx}}}$ y $t_{(n-2)}=\dfrac{\hat{\beta}_1-\beta_1}{ \hat{\sigma} \sqrt{1/S_{xx}}}$ tienen distribuciones $t$ con $n-2$ grados de libertad. De esto se deduce que los intervalos de confianza de $100*(1-\alpha)\%$ para la pendiente $\beta_1$ y el intercepto $\beta_0$ de la línea de regrasión verdadera son: $$\hat{\beta}_0 \pm t_{\alpha/2, n-2} \cdot  \hat{\sigma} \sqrt{1/n+\bar{x}^2/S_{xx}}\hspace{10mm}(9)$$ $$\hat{\beta}_1 \pm t_{\alpha/2, n-2} \cdot  \hat{\sigma} \sqrt{1/S_{xx}} \hspace{10mm}  (10)$$estos intervalos están centarados en la en la estimación puntual de cada parámetro y la cantidad abarcada a cada lado de la estimación depende del nivel de confianza deseado y de la cantidad de variabilidad del estimador.

Dado lo anterior, para los procedimientos de prueba de hipótesis, y como se procede habitualmente, las hipótesis nulas respecto a los beta del modelo de regresión lineal simple serán enunciados de igualdad. Los valores nulos para $\beta_0$ y $\beta_1$ se representan respectivamente como $\beta_{00}$ ("beta cero cero") y $\beta_{10}$ ("beta uno cero"). Además, como los estadísticos de prueba tienen distribuciones $t$ con $n-2$ grados de libertad cuando $H_0$ es verdadera, la probabilidad de error Tipo I permanece al nivel deseado $\alpha$ usando un valor crítico $t$ adecuado. Así, las hipótesis comúnmente usadas para $\beta_0$son: $$H_0: \beta_0 = \beta_{00}\hspace{10mm}(11)$$ $$H_1: \beta_0 \neq \beta_{00}\hspace{10mm}(12)$$cuyo estadístico de prueba es: $$t_{(n-2)}=\dfrac{\hat{\beta}_0-\beta_{00}}{\hat{\sigma} \sqrt{1/n+\bar{x}^2/S_{xx}}}\hspace{10mm}(13)$$y para $\beta_1$ son:
$$H_0: \beta_1 = \beta_{10}\hspace{10mm}(14)$$ 
$$H_1: \beta_1 \neq \beta_{10}\hspace{10mm}(15)$$cuyo estadístico de prueba es:$$t_{(n-2)}=\dfrac{\hat{\beta}_1-\beta_{10}}{\hat{\sigma} \sqrt{1/S_{xx}}}\hspace{10mm}(16)$$el par de hipótesis definidas por $14$, $15$ y $16$ se conoce como la prueba de utilidad del modelo de regresión lineal simple, donde: la región de rechazo de $H_0$ para una prueba a nivel $\alpha$ a favor de $H_1: \beta_1>\beta_{10}$ es $t\geq t_{\alpha,n-2}$; la región de rechazo de $H_0$ para una prueba a nivel $\alpha$ a favor de $H_1: \beta_1<\beta_{10}$ es $t\leq -t_{\alpha,n-2}$; y la región de rechazo de $H_0$ para una prueba a nivel $\alpha$ a favor de $H_1: \beta_1\neq\beta_{10}$ es $t\leq -t_{\alpha/2,n-2}$ o  $t\geq t_{\alpha/2,n-2}$. Además, se sabe que la prueba de utilidad del modelo de regresión simple puede ser probada con una tabla ANOVA: rechazando $H_0$ si $f\geq F_{\alpha,1,n-2}$. La prueba $F$ da exactamente el mismo resultado que la prueba $t$ de utilidad del modelo de regresión lineal simple.

Por último, se entiende que en un modelo de regresión lineal simple un valor futuro de $Y$ no es parámetro sino una variable aleatoria, por lo que se debe hacer referencia a un intervalo de valores factibles para un valor futuro de $Y$, al cual se le llama intervalo de predicción. Cuando se predice con base en el modelo de regresión lineal simple, el error de predicción es $Y-( \hat{\beta}_0+ \hat{\beta}_1 x^*)$ que corresponde con una diferencia entre dos variables aleatorias, por lo que, en comparación con una estimación, habrá más incertidumbre en ese; por lo tanto, un intervalo de predicción será más ancho que un intervalo de confianza. Además, a partir de la varianza del error dde predicción se puede establecer que la variable estandarizada:$$T=\dfrac{Y-(\hat{\beta}_0+ \hat{\beta}_1 x^*)}{S \displaystyle\sqrt{1+\dfrac{1}{n}  + \dfrac{(x^*-\bar{x})^2}{S_{xx}}}}\hspace{10mm}(17)$$tiene una distribución $t$ con $n-2$ grados de libertad, a partir de la cual se obtine un intervalo de predicción de $100*(1-\alpha)\%$ para una observación $Y$ futura que se hará cuando $x=x^*$ igual a:$$\hat{\beta}_0+\hat{\beta}_1 x^*\pm t_{n-2,\alpha/2}\cdot s \displaystyle\sqrt{1+\dfrac{1}{n}+\dfrac{(x^*-\bar{x})^2}{S_{xx}}}\hspace{10mm}(18)$$ la interpretación del nivel de predicción de $100*(1-\alpha)\%$ establece que al usar $(18)$ repetidamente, los intervalos resultantes contendrán los valores $y$ observados el $100*(1-\alpha)\%$ del tiempo. Además, el número $1$ en la raíz cuadrada hace que el intervalo de predicción sea más ancho que intervalos de confianza como $(9)$ y $(10)$. Asimismo, a medida que $n\to\infty$ el ancho del intervalo no-tiende a cero, porque la incertidumbre en la predicción será permanente, incluso al tener conocimiento perfecto sobre $\beta_0$ y $\beta_1$.

#### 3.1.1. Planteamiento del Problema
Con base en el conjunto de datos descrito en la sección **2** se formulará un modelo de regresión lineal simple para estudiar la relación lineal supuesta entre las varaibles definidas por los campos: **Chance of Admit** (variable dependiente) y **TOEFL Score** (variable independiente).

#### 3.1.2. Desarrollo del Análisis
El estudio de regresión lineal simple ha sido procesado con `r R.version.string` mediado por RStudio 2022.07.2 Build 576 en una plataforma x86_64-w64-mingw32.

##### 3.1.2.1. Resumen estadístico de las variables por estudiar. {.tabset .tabset-pills}
La navegación a través de las pestañas muestra el resumen estadístico de las variables de interés: **Chance of Admit** (variable dependiente) y **TOEFL Score** (variable independiente), junto con sus respectivos diagramas de caja. Además, incluye el diagrama de dispersión de sus valores conjuntos.

Con base en la pestaña **Resumen de Chance of Admit** se puede comentar que la variable **Chance of Admit** presenta asimetría de sesgo negativo con rango intercuartílico estrecho de dispersión imperceptible. Además, un dato se visualiza como outlier. Así, puede decirse que la variable registra valores altos en relación con su intervalo de medición. En comparación, según la pestaña **Resumen de TOEFL Score**, la variable **TOEFL Score** se visualiza más simétrica que la anterior, sin presentar datos atípicos, pero con mediana que muestra una ligera concentración de medidas hacia la mitad superior de los datos.

Complementariamente, en **Diagrama de Dispersión TOEFL vs. CoA** se puede observar que existe una correlación positiva de naturaleza apreciablemente lineal entre las variable **Chance of Admit** y **TOEFL Score**. Sin embargo, si se observa el gráfico de **Diagramas Totales de Dispersión** (que excluyeron las variables cualitativas::nominales) es constatable que existe una correlación más fuerte entre la variable de interés **Chance of Admit** y **CGPA**.

###### Resumen de Chance of Admit
```{r resumen_Chance_of_Admit, fig.align = 'center'}
summary(Admission_Dataset$Chance_of_Admit)
boxplot(Admission_Dataset$Chance_of_Admit, main = "Diagrama de Caja de Chance of Admit", col = c("orange"))
```

###### Resumen de TOEFL Score
```{r resumen_TOEFL_Score, fig.align = 'center'}
summary(Admission_Dataset$TOEFL_Score)
boxplot(Admission_Dataset$TOEFL_Score, main = "Diagrama de Caja de TOEFL Score", col = c("gold"))
```

###### Diagrama de Dispersión TOEFL vs. CoA 
```{r dispersion_TOEFL_Chance, fig.align = 'center'}
plot(Admission_Dataset$TOEFL_Score, Admission_Dataset$Chance_of_Admit, main = "Diagrama de Dispersión TOEFL Score vs. Chance of Admit")
```

###### Diagramas Totales de Dispersión
```{r dispersiones_Totales, fig.align = 'center'}
pairs(~GRE_Score + TOEFL_Score + SOP + LOR + CGPA + Chance_of_Admit, data = Admission_Dataset)
```

##### 3.1.2.2. Formulación del modelo de RLS entre las variables de estudio. {.tabset .tabset-pills}
La navegación a través de las pestañas muestra los coeficientes del modelo de regresión lineal simple, su resumen estadístico y su tabla ANOVA. Se menciona de nuevo que las variables de interés son: **Chance of Admit** (variable dependiente) y **TOEFL Score** (variable independiente).

Al considerar los resultados presentados en la pestaña **Coeficientes del Modelo RLS** se puede establer que el modelo de regresión lineal simple que relaciona a las variables de interés, las cuales se resumiran como $COA$ y $TOEFL$, tiene la formulación:$$\hat{COA}=-1.2734005+0.0185993\cdot TOEFL\hspace{10mm}(19)$$ para este modelo se obvia la interpretación del inteercepto por carecer de sentido dado que **Chance of Admit** resultaría negativa en caso de un valor nulo de **TOEFL Score**, y ambas situaciones carecen de sentido. Sin embargo, el coeficiente lineal una correlación de proporcionalidad directa entre las variables de interés, aunque de crecimiento moderado en **Chance of Admit** por cada unidad marginal de **TOEFL Score**.

Complementariamente, en la pestaña **Resumen Estadístico del Modelo RLS** se constata que para cualquier nivel de significancia las evidencias estarán a favor de la correlación positiva entre las variables de interés. Además, el coeficiente de detreminación está a favor de la correlación estableciendo que el $62.57$ $\%$ de la variabilidad de **Chance of Admit** es explicada por **TOEFL Score**, esto confirmado también a través de la pestaña **Tabla ANOVA para el Modelo RLS**.

###### Coeficientes del Modelo RLS
```{r coeficientes_Regresion_Lineal_Simple, fig.align = 'center'}
modelo_RL_Simple = lm(Admission_Dataset$Chance_of_Admit~Admission_Dataset$TOEFL_Score)
coef(modelo_RL_Simple)
```
###### Resumen Estadístico del Modelo RLS
```{r resumen_Regresion_Lineal_Simple, fig.align = 'center'}
summary(modelo_RL_Simple)
```
###### Tabla ANOVA para el Modelo RLS
```{r ANOVA_Regresion_Lineal_Simple, fig.align = 'center'}
anova(modelo_RL_Simple)
```

##### 3.1.2.3. Análisis del modelo RLS. {.tabset .tabset-pills}
La navegación a través de las pestañas muestra el intervalo de confianza para $\beta_1$ y para la predicción del modelo de regresión lineal simple, ambos al 95 %. Se menciona de nuevo que las variables de interés son: **Chance of Admit** (variable dependiente) y **TOEFL Score** (variable independiente).

El análisis del modelo RLS muestra que es significativo y en consecuencia aporta información relevante para estimar **Chance of Admit** a partir de **TOEFL Score**. Esto debido a que el intervalo de confianza para el coeficiente de *TOEFL* en el modelo RLS excluye al cero:$$0.01718449<\beta_1<0.02001411\hspace{10mm}(20)$$

Por último, la pestaña **Predicciones y sus Intevalos de Predicción** muestran los cálculos con base en el modelo, bajo intervalos de predicción al $95$ $\%$, de las predicciones de todas las pestañas del conjunto de datos para la variable **Chance of Admit**. Cabe mencionar que estos intervalos resultan más anchos que aquellos calculados con base en intervalos de confianza al mismo nivel de significancia.

###### Intervalo de Confianza para B1
```{r Intervalo_Confianza_B1, fig.align = 'center'}
confint(modelo_RL_Simple, level = 0.95)
```
###### Predicciones y sus Intervalos de Predicción
```{r Intervalos_Prediccion_P, fig.align = 'center'}
predict(modelo_RL_Simple, data.frame(seq(1,400)), interval='prediction', level = 0.95)
```
###### Predicciones y sus Intervalos de Confianza
```{r Intervalos_Prediccion_C, fig.align = 'center'}
predict(modelo_RL_Simple, data.frame(seq(1,400)), interval='confidence', level = 0.95)
```

### 3.2. Regresión Lineal Múltiple
Este modelo, que eventualmente será llamado en este estudio como **RLM**, establece que:...

#### 3.2.1. Planteamiento del Problema
Con base en el conjunto de datos descrito en la sección **2** se formulará un modelo de regresión lineal múltiple para estudiar la relación lineal múltiple supuesta entre las varaibles definidas por los campos: **Chance of Admit** (variable dependiente) y los demás como variables independientes: **Gender**, **GRE Score**, **TOEFL Score**, **SOP**, **LOR**, **CGPA**, **Research** y **University Rating**.

#### 3.2.2. Desarrollo del Análisis
El estudio de regresión lineal múltiple ha sido procesado con `r R.version.string` mediado por RStudio 2022.07.2 Build 576 en una plataforma x86_64-w64-mingw32.

##### 3.2.2.1. Resumen estadístico de las variables de estudio. {.tabset .tabset-pills}
La navegación a través de las pestañas muestra el resumen estadístico de todas las variables del conjunto de datos, excepto **Serial**, porque simplemente es un índice posicional. Sin embargo, para las varaibles de naturaleza cuantitativa::razón el resumen será el tradicional, pero para las variables de naturaleza cualitativa::nominal el resumen estadístico consistirá en conteos, proporciones y diagramas de barras.  Se menciona de nuevo que **Chance of Admit** es la variable dependiente.

###### Resumen Variables Cuantitativas
```{r resumen_Variables_Cuantitativas, fig.align = 'center'}
summary(Admission_Dataset$GRE_Score)
summary(Admission_Dataset$TOEFL_Score)
summary(Admission_Dataset$SOP)
summary(Admission_Dataset$LOR)
summary(Admission_Dataset$CGPA)
summary(Admission_Dataset$Chance_of_Admit)
```

###### Resumen Variables Cualitativas
```{r resumen_Variables_Cualitativas, fig.align = 'center'}
table(Admission_Dataset$Gender)
prop.table(table(Admission_Dataset$Gender))
barplot(table(Admission_Dataset$Gender))
table(Admission_Dataset$Research)
prop.table(table(Admission_Dataset$Research))
barplot(table(Admission_Dataset$Research))
table(Admission_Dataset$University_Rating)
prop.table(table(Admission_Dataset$University_Rating))
barplot(table(Admission_Dataset$University_Rating))
```

###### Diagramas de Dispersión Variables Cuantitativas
```{r dispersiones_Variables_Cuantitativas, fig.align = 'center'}
pairs(~GRE_Score + TOEFL_Score + SOP + LOR + CGPA + Chance_of_Admit, data = Admission_Dataset)
```

##### 3.2.2.2. Formulación del modelo de RLM entre las variables de estudio. {.tabset .tabset-pills}
La navegación a través de las pestañas muestra los resúmenes de un par de modelos de regresión lineal múltiple, sus resúmenes estadísticos y sus tablas ANOVA. Con base en la exploración de los datos de la sesión 3.2.2.1. se formulan para comparación dos modelos RLM: uno que incluye a todas las varibles del conjunto de datos, excepto **Serial**, y otro que excluye a **Gender**, **SOP** y **University Rating**. Se menciona de nuevo que **Chance of Admit** es la variable dependiente.

###### Resumen Estadístico del Modelo RLM Total
```{r, resumen_RLM_Total, fig.align = 'center'}
summary(lm(Admission_Dataset$Chance_of_Admit~Admission_Dataset$GRE_Score+Admission_Dataset$TOEFL_Score+Admission_Dataset$SOP+Admission_Dataset$LOR+Admission_Dataset$CGPA+as.factor(Admission_Dataset$Gender)+as.factor(Admission_Dataset$Research)+as.factor(Admission_Dataset$University_Rating)))
```

###### ANOVA del Modelo RLM TOTAL
```{r}
anova(lm(Admission_Dataset$Chance_of_Admit~Admission_Dataset$GRE_Score+Admission_Dataset$TOEFL_Score+Admission_Dataset$SOP+Admission_Dataset$LOR+Admission_Dataset$CGPA+as.factor(Admission_Dataset$Gender)+as.factor(Admission_Dataset$Research)+as.factor(Admission_Dataset$University_Rating)))
```

###### Resumen Estadístico del Modelo RLM Reducido
```{r resumen_RLM_Reducido, fig.align = 'center'}
summary(lm(Admission_Dataset$Chance_of_Admit~Admission_Dataset$GRE_Score+Admission_Dataset$TOEFL_Score+Admission_Dataset$LOR+Admission_Dataset$CGPA+as.factor(Admission_Dataset$Research)))
```

###### ANOVA del Modelo RLM Reducido
```{r}
anova(lm(Admission_Dataset$Chance_of_Admit~Admission_Dataset$GRE_Score+Admission_Dataset$TOEFL_Score+Admission_Dataset$LOR+Admission_Dataset$CGPA+as.factor(Admission_Dataset$Research)))
```

##### 3.2.2.3. Análisis del modelo RLM. {.tabset .tabset-pills}
Bondad de ajuste, significancia global (F), significancia individual (t), criterio de información de Akaike (AIC), mejor modelo en términos del AIC (función STEP).

###### Mejor Modelo Iterado según AIC
```{r, mejor_Modelo_AIC, fig.align = 'center'}
modelo_Iterado_STEP = step(lm(Admission_Dataset$Chance_of_Admit~Admission_Dataset$GRE_Score+Admission_Dataset$TOEFL_Score+Admission_Dataset$SOP+Admission_Dataset$LOR+Admission_Dataset$CGPA+as.factor(Admission_Dataset$Gender)+as.factor(Admission_Dataset$Research)+as.factor(Admission_Dataset$University_Rating)))
coefficients(modelo_Iterado_STEP)
```

###### Bondades, Significancias y Criterios de Información Comparados
```{r}
modelo_RLM_TOTAL = lm(Admission_Dataset$Chance_of_Admit~Admission_Dataset$GRE_Score+Admission_Dataset$TOEFL_Score+Admission_Dataset$SOP+Admission_Dataset$LOR+Admission_Dataset$CGPA+as.factor(Admission_Dataset$Gender)+as.factor(Admission_Dataset$Research)+as.factor(Admission_Dataset$University_Rating))
modelo_RLM_REDUCIDO = lm(Admission_Dataset$Chance_of_Admit~Admission_Dataset$GRE_Score+Admission_Dataset$TOEFL_Score+Admission_Dataset$LOR+Admission_Dataset$CGPA+as.factor(Admission_Dataset$Research))

stargazer(modelo_RLM_TOTAL, modelo_RLM_REDUCIDO, modelo_Iterado_STEP, type = "text", df = TRUE)
AIC(modelo_RLM_TOTAL, modelo_RLM_REDUCIDO, modelo_Iterado_STEP)
BIC(modelo_RLM_TOTAL, modelo_RLM_REDUCIDO, modelo_Iterado_STEP)
```

### 3.3. Regresión Logística Simple
Este modelo, que eventualmente será llamado en este estudio como **RLogS**, establece que, en comparación con un modelo de regresión lineal simple que relacione una variable cuantitativa dependiente $y$ con una varaible cuantitativa independiente $x$, relaciona una variable categórica dicotómica (con valores posibles $1$ (éxito) y $0$ (fracaso)) dependiente $y$ con el valor de probabilidad $p(x)\in [0, 1]$ que depende de alguna variable cuantitativa $x$.

Como se mencionó en la sección **1** los modelos de regresión usados en este estudio pueden ser vistos como casos particulares del Modelo Lineal Generalizado (**GLM** por sus siglas en inglés). Este modelo extiende al modelo lineal general al lograr que la variable dependiente está relacionada linealmente con sus factores y covariables a través de alguna función de enlace y que la variable dependiente pueda tener una distribución diferente a la normal. Además de los modelos usados en este estudio, el **GLM** también cubre: modelos loglineales para datos de recuento, modelos log-log complementario para datos de supervivencia censurados por intervalos, y otros modelos estadísticos a través de la propia formulación general del modelo.

Como **GLM** permite especificar distribuciones diferentes a la normal y una función de enlace (entendida como una transformación de la variable dependiente que permite la estimación del modelo) diferente a la identidad se puede trabajar con muchas combinaciones posibles de distribuciones y funciones de enlace, varias de las cuales pueden ser adecuadas para un conjunto de datos en particular, esto implica que la elección de la combinación estará orientada por consideraciones teóricas a priori, por la naturaleza de las variables, la experiencia del investigador y los resultados al comparar combinaciones.

En el caso por tratar, se trabajará con base en una distribución binomial (adecuada para variables que representan una respuesta binaria) con función de enlace logit:$$\pi(x)=\dfrac{e^{\beta_0+\beta_1 x}}{1+ e^{\beta_0 +\beta_1 x}}=  \dfrac{1}{1+ e^{-(\beta_0+\beta_1 x)}}\hspace{10mm}(--)$$(del inglés **log**arithmic un**it**: unidad logarítmica (natural)); que además es apropiada únicamente para la distribución binomial), por lo cual un nombre más adecuado para la regresión podría ser regresión logística binaria. Cabe anotar que el término logístico hace referencia a que la función de enlace constituye, en cierto sentido, un refinamiento del modelo exponencial de crecimiento, descrito por la función sigmoidea, de una magnitud asociada con un conjunto $C$.

Para facilitar las interpretaciones se entiende que la función de enlace $\pi(x)$ proviene de una razón de probabilidades (conocida en idioma inglés como ODDS ratio (OR)), que a su vez es el argumento de un logaritmo: $\log\left(\frac{\pi(x)}{1-\pi(x)}\right)$, así, se modela la probabilidad de que la variable de respuesta pertenezca al nivel de referencia $1$ en función del valor de los predictores. Complementariamente, la transformación de probabilidades a razones de probabilidad es conserva la monotonicidad de sentidos. Además, la transformación convierte el intervalo de probabilidad $[0,1]$ a $(-\infty,\infty)$. Las propiedades que se dan entre las probabilidades complementarias de éxito y fracaso, sus razones y la función de enlace logit son:

||||
:-:|:-:|:-:|
$p(éxito)=p(fracaso)$|$OR=1$|$Logit\left(OR\right)=0$
$p(éxito)<p(fracaso)$|$OR<1$|$Logit\left(OR\right)<0$
$p(éxito)>p(fracaso)$|$OR>1$|$Logit\left(OR\right)>0$
||||

Se entiende que la transformación $Logit$ carece de sentido para la certeza del éxito o del fracaso.

#### 3.3.1. Planteamiento del Problema
Con base en el conjunto de datos descrito en la sección **2** se formulará un modelo de regresión logística simple para estudiar la relación logística supuesta entre las varaibles definidas por los campos: **Chance of Admit** (variable independiente) y **Research** (variable dependiente), con base en una distribución binomial y la función de enlace $Logit$.

#### 3.3.2. Desarrollo del Análisis
El estudio de regresión lineal simple ha sido procesado con `r R.version.string` mediado por RStudio 2022.07.2 Build 576 en una plataforma x86_64-w64-mingw32.

##### 3.3.2.1. Resumen estadístico de las variables de estudio. {.tabset .tabset-pills}
La navegación a través de las pestañas muestra el resumen estadístico de la variable independiente **Chance of Admit**, su boxplot e histograma. De la variable dependiente **Research**
se mostrará su diagrama de barras, así como su media y mediana. Además, se exhibirá un Diagrama de Cajas conjunto entre aquellas.

Con base en la pestaña **Resumen y Boxplot de Chance of Admit** se puede comentar que la variable **Chance of Admit**, como se hizo en la sección 3.1.2.1., presenta asimetría de sesgo negativo con rango intercuartílico estrecho de dispersión imperceptible. Además, un dato se visualiza como outlier. Así, puede decirse que la variable registra valores altos en relación con su intervalo de medición. Lo expuesto también es constatable a través de la pestaña **Histograma de Chance of Admit**.

Asimismo, según la pestaña **Resumen y Diagrama de Barras de Research**, la variable cualitativa::nominal **Research** muestra proporcionalidad mayor para los casos favorables 1, que para los casos desfavorables 0: $54.75$ $\%$ y $45.25$ $\%$, respectivamente.

Complementariamente, la pestaña **Resumen y Diagrama de Cajas Conjunto** muestra que las observaciones son consistentes con el contexto del problema; es decir, para los casos favorables de **Research** los resultados de **Chance of Admit** son mayores, en comparación con los casos desfavorables. Además, ambos casos: los favorables y los desfavorables, muestran sesgo negativo, aunque es más notorio para los casos favorable. También, en el rango intercuartílico se visualizan dispersiones opuestas entre los casos desfavorables, hacia la mitad inferior de los datos, y los casos favorables, hacia la mitad superior. Asimismo, los atípicos, en ambos casos, se presentan en los extremos inferiores de las distribuciones, es decir, resultaron más extraños los valores bajos.

###### Resumen y Boxplot de Chance of Admit
```{r resumen_Chance__of__Admit, fig.align = 'center'}
summary(Admission_Dataset$Chance_of_Admit)
boxplot(Admission_Dataset$Chance_of_Admit, main = "Diagrama de Caja de Chance of Admit", col = c("orange"))
```

###### Histograma de Chance of Admit
```{r histograma_Chance_of_Admit, fig.align = 'center'}
summary(Admission_Dataset$Chance_of_Admit)
hist(Admission_Dataset$Chance_of_Admit, main = "Histograma de Chance of Admit", col = c("gold"))
```

###### Resumen y Diagrama de Barras de Research
```{r resumen_Research, fig.align = 'center'}
table(Admission_Dataset$Research)
prop.table(table(Admission_Dataset$Research))
barplot(table(Admission_Dataset$Research))
```

###### Resumen y Diagrama de Cajas Conjunto
```{r boxplot_Conjunto_Chance-of-Admit_Research, fig.align = 'center'}
tapply(Admission_Dataset$Chance_of_Admit, Admission_Dataset$Research, mean)
tapply(Admission_Dataset$Chance_of_Admit, Admission_Dataset$Research, median)
boxplot(Admission_Dataset$Chance_of_Admit~Admission_Dataset$Research, main = "Boxplot Conjunto: Chance of Admit - Research", col = c("orange", "gold"))
```

##### 3.3.2.2. Formulación del modelo de RLogS entre las variables de estudio. {.tabset .tabset-pills}
La navegación a través de las pestañas muestra los coeficientes del modelo RLogS y su resumen estadístico. Se menciona de nuevo que las variables de interés son: **Chance of Admit** (variable independiente) y **Research** (variable dependiente).

La pestaña **Coeficientes del Modelo RLogS** permite establecer que el modelo RLogS relaciona a $\pi(x)$ con $x$ a través de la función de enlace $Logit$ de la siguiente manera:$$\frac{\pi(x)}{1-\pi(x)}=e^{-7.658709+10.886959\cdot x}$$

Asimismo, la pestaña **Resumen Estadístico del Modelo RLogS** muestra, para efectos de comparación, los resúmenes del modelo estudiado y de uno alternativo con base en la variable cualitativa::nominal **Gender**. Con base en el criterio de información de Akaike (AIC por sus siglas en inglés), del cual se sabe que es una medida de la bondad de ajuste de un modelo estadístico que describe la relación entre el sesgo y la varianza en la formulación del modelo, es decir, entre su exactitud y complejidad, se verifica que resulta un mejor modelo de la variable **Research** que de la variable **Gender**, porque: $AIC_R=412.60<534.09=AIC_G$. También, para apoyar que el modelo basado en la variable **Research** es mejor que aquél basado en **Gender**, el cociente entre la desviación nula (Null Deviance) y la desviación residual (Residual Deviance), observable en la pestaña Resumen Estadístico del Modelo RLosgS, es mayor en el modelo propuesto que en el de comparación.

###### Coeficientes del Modelo RLogS
```{r coeficientes_Regresion_Logistica_Simple, fig.align = 'center'}
modelo_RLog_Simple = glm(Admission_Dataset$Research~Admission_Dataset$Chance_of_Admit, family = "binomial", data = data.frame(Admission_Dataset$Research, Admission_Dataset$Chance_of_Admit))
coef(modelo_RLog_Simple)
```
###### Resumen Estadístico del Modelo RLogS
```{r resumen_Regresion_Logistica_Simple, fig.align = 'center'}
summary(modelo_RLog_Simple)
modelo_RLog_Simple_S = glm(Admission_Dataset$Gender~Admission_Dataset$Chance_of_Admit, family = "binomial", data = data.frame(Admission_Dataset$Gender, Admission_Dataset$Chance_of_Admit))
summary(modelo_RLog_Simple_S)
```

##### 3.3.2.3. Análisis del modelo RLogS. {.tabset .tabset-pills}
Se mostrarán, a través de pestañas, los resultados de algunas predicciones obtenidas a través del modelo RLogS para identificar en sus respuestas la correspondencia de sentido en las razones de probabilidades ODDS a favor o en contra del evento considerado: $\frac{\pi}{1-\pi}$ y $\frac{1-\pi}{\pi}$, respectivamente. Se menciona de nuevo que las variables de interés son: **Chance of Admit** (variable independiente) y **Research** (variable dependiente).

La pestaña **Variable Predictora igual a Cero** plantea dos situaciones interpretativas. La primera, permite comprender que el coeficiente del factor en el cual está presente la varaible predictora, estima una probabilidad de caso favorable cercana a cero en el orden de la diezmilésima. La segunda, acarrea una interpretación más delicada: como la variable **Chance of Admit** se mide en el intervalo $[0,1]$ con dos cifras significativas y dos decimales de precisión, una unidad de medida razonable en ella sería una centésima, por ejemplo: pasar de $0.62$ a $0.63$ implica incrementar en una unidad de medida a $0.62$. Así, se entiende que el cociente de probabilidades en relación con la variable predictora en el modelo RLogS refleje un incremento acumulado de $\approx 53474.4$ veces desde $0$ hasta $1$, con incrementos de $0.01$.

Con base en lo anterior, a través de la pestaña **Probabilidades Estimadas** se puede apreciar entre los registros $19$ y $20$ un delta de cambio absoluto igual a $0.022795$ (equivalente a un incremento relativo $\approx 7.94$ $\%$) al incrementarse la variable predictora en una unidad como se definió en el párrafo anterior.

Por último, el gráfico de curva logística, en la pestaña Gráfica del Modelo RLogS, permite visualmente comprender el comportamiento de las variables involucradas en el modelo propuesto; es decir, los casos favorables en relación con la variable **Research** logran superar la barrera del $90$$\%$ del valor de **Chance of Admit**.

###### Variable Predictora igual a Cero
```{r variable_Predictora_Cero, fig.align = 'center'}
coef(modelo_RLog_Simple)
round(exp(coef(modelo_RLog_Simple)),6)
```
###### Probabilidades Estimadas
```{r probabilidades_Estimadas, fig.align = 'center'}
predict(modelo_RLog_Simple, data.frame(seq(1, 400)), type = "response")
```
###### Gráfica del Modelo RLogS
```{r grafica_Modelo_RLogS, fig.align = 'center'}
research <- Admission_Dataset$Research
chance_of_admit <- Admission_Dataset$Chance_of_Admit
dataPlot <- data.frame(chance_of_admit, research)
plot(research~chance_of_admit, data = dataPlot, main = "Modelo RLogS: Chance of Admit - Research", xlab = "Chance of Admit", ylab = "Research = 0 | Research = 1", col = "gold", pch = "I")
curve(predict(glm(research~chance_of_admit, family = "binomial", data = dataPlot), data.frame(chance_of_admit = x), type = "response"), col = "orange", lwd = 3, add = TRUE)
```

## **Conclusiones**

## **Referencias**
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0px;"></div>